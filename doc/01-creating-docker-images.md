cd# Building Docker images for applications

Each individual application used in the tutorial needs to be packaged as a Docker container. Container packaging methodology can vary widely, but for the purposes of this tutorial we will package a set of bioinformatics tools to achieve the following goals:

1. A readable and maintanable Dockerfile
2. Leverage a base image that satisfies most dependencies across a set of applications.
3. Small and long-lived container images
4. Provide an application entry point to both expose and limit the default functionality of the container image
5. Account for job multitenancy on the underlying Amazon EC2 instances containers are deployed to the instances
6. Stage and save data to and from the tasks and Amazon S3

## The Dockerfile

Your Dockerfile contains all of the commands that you use to package your Docker container. In it, you pick a base image to build from, include any metadata to attribute to the image, describe how to build and configure the environment, and how to access the code running within it.

We recommend that you adopt a standard set of conventions for your Dockerfiles, such as sections for:

* Metadata to describe the contained application
* Prerequisites packages from the operating system release
* Runtime prerequisites installed through the runtime's package management (e.g. python eggs)
* The application
* Execution wrappers and other support tools for the application

The `[src/tools](../src/tools/)` directory contains all the necessary artifacts that are needed for the Docker containers we will build for each tool, including a Dockerfile formatted with all of the above. Below is an example for the [SAMtools](http://www.htslib.org).  Prior to installing SAMtools please review the license [here](https://github.com/samtools/samtools/blob/develop/LICENSE) and verify it is acceptable to you for your use.

Here is the Dockerfile for SAMtools:

```Dockerfile
FROM python:2.7

# Metadata
LABEL container.base.image="python:2.7"
LABEL software.name="SAMtools"
LABEL software.version="1.5"
LABEL software.description="Utilities for the Sequence Alignment/Map (SAM/BAM/CRAM) formats"
LABEL software.website="http://www.htslib.org"
LABEL software.documentation="http://www.htslib.org/doc/samtools.html"
LABEL software.license="MIT/Expat"
LABEL tags="Genomics"

# System and library dependencies
RUN apt-get -y update && \
    apt-get -y install unzip gcc libncurses5-dev && \
    apt-get clean

# Other software dependencies
RUN pip install boto3 awscli

# Application installation
RUN wget -O /samtools-1.4.tar.bz2 \
  https://github.com/samtools/samtools/releases/download/1.5/samtools-1.5.tar.bz2 && \
  tar xvjf /samtools-1.5.tar.bz2 && rm /samtools-1.5.tar.bz2

RUN cd /samtools-1.5 && make && make install

# Application entry point
COPY samtools_stats/src/run_samtools_stats.py /run_samtools_stats.py
COPY common_utils /common_utils

ENTRYPOINT ["python", "/run_samtools_stats.py"]
```


## Job multitenancy and sharing data between jobs
Many bioinformatics tools have been developed to run in any Linux environment, and not necessarily optimized for cloud computing or multitenancy. To overcome these challenges, you can use a simple Python wrapper script for each tool that facilitates the deployment of a job. In the Dockerfile above you will see the structure we proposed, as well as mention of the wrapper script for bootstrapping an application and staging data to/from S3.

All of the applications in this tutorial have several of the same requirements, such as the need to read and write from S3 and deal with job multitenancy. For these common utilities, we built a separate [`common_utils`](../tools/common_utils) package to import during the creation of the Docker image. These utilities deal with the previously mentioned common requirements, such as:

### Container placement

To make your workflow as flexible as possible, each job should run independently. As a result, you cannot necessarily guarantee that different jobs in the same overall workflow run on the same instance. Using S3 as the location to exchange data between containers enables you to decouple storage of your intermediate files from compute. The [`tools/common_utils/s3_utils.py`](../tools/common_utils/s3_utils.py) script contains the functions required to leverage S3.

### Multitenancy

Multiple container jobs may run concurrently on the same instance. In these situations, it’s essential that your job writes to a unique subdirectory. An easy way to do this is to create a subfolder using a UUID and have your application write all of your data there. This is implemented in the [`tools/common_utils/job_utils.py`](../tools/common_utils/job_utils.py) module.

### Cleanup

As your jobs complete and write the output back to S3, you can delete the scratch data on your instance generated by that job. This allows you to optimize for cost by reusing EC2 instances if there are jobs remaining in the queue, rather than terminating the EC2 instances. As you ensure that you’re writing to a unique subdirectory in the multitenancy solution, you can simply delete that subdirectory to minimize your storage footprint. This is also implemented in [`tools/common_utils/job_utils.py`](../tools/common_utils/job_utils.py).

### Runtime application dependencies

Each of the Python wrappers takes in all of the requisite data dependencies, residing in S3, as command-line arguments and any other necessary commands to run the tool it wraps. It then handles all of the file downloading, running the bioinformatics tool, and uploading the files back to S3. For more information about each of these tools, see the READMEs for each individual tool. The one for SAMtools is [here](../tools/samtools_stats/README.md).

An example for the Isaac wrapper script is at   [`tools/samtools_stats/src/run_samtools_stats.py`](../tools/isaac/src/run_isaac.py).

## Building and deploying the Docker images

We have provided a Makefiles to build all of the necessary Docker images for this demonstration project, as well as deploy each container to a AWS [EC2 Container Registry](https://aws.amazon.com/ecr/) ([ECR](https://aws.amazon.com/ecr/)). You can review the SAMtools example [here](../tools/samtools_stats/docker/Makefile).

To build and upload the container image for Isaac, execute the following commands in a terminal:

```sh
# From the root of the repository directory
cd tools/samtools_stats/docker

# Login to ECR so that docker can submit the image to the ECR repository
# NOTE: The --no-include-email flag is only needed for docker version >17.06
eval $(aws ecr get-login --no-include-email)

# Create a ECR repository for Isaac, then copy the `repositoryUri` into a variable
REPO_URI=$(aws ecr create-repository --repository-name samtools_stats \
  --output text --query "repository.repositoryUri")

# NOTE: If the 'isaac' repository already exists, then query ECR for the `repositoryUri` instead like so:
REPO_URI=$(aws ecr describe-repositories \
  --repository-names samtools_stats \
  --output text --query "repositories[0].repositoryUri")

# execute the build supplying the repositoryUri
make REGISTRY=${REPO_URI}
```

The above commands create the Amazon ECR repository to push the created Docker container image to, builds the image, then pushes the image to ECR.

Make a note of the `repositoryUri` for use in a later step.

## Rinse and repeat

You will now need to accomplish the same build process for the other tools:

* Isaac (license [here](https://github.com/Illumina/Isaac3/blob/master/COPYRIGHT))
* Strelka (license [here](https://github.com/Illumina/strelka/blob/master/LICENSE.txt))
* SnpEff (license is LGPLv3, as referenced [here](http://snpeff.sourceforge.net/download.html)).

As above, make sure to note down the `repositoryUri` of each tool for use in later steps.

## PROTIP: Make a smaller Isaac image

The Isaac build process has the potential to create a very large image. To get a smaller image size you can can squash the image layers, you can export/import the image using docker like so:

```sh
# use the repositoryUri from the ECR repository for REPO_URI
make build REGISTRY=${REPO_URI}
docker run -d isaac:latest
# command will return the SHA256 hash of the container which will look like:
# 0343ffa66ef88a32b502b349aa1c8b2aefc90dec9f201b4bfcee61b551982131

# copy enough of the hash to identify the container then export and import
docker export 0343ffa66ef8 > isaac_squash.tar
docker import isaac_squash.tar isaac:latest

# write the other tags
make tag REGISTRY=${REPO_URI}

# push up to ECR
make push REGISTRY=${REPO_URI}

# optional: clean up after yourself
rm isaac_squash.tar
```
